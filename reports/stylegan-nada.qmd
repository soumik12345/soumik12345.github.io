---
title: "Digging Into StyleGAN-NADA for CLIP-Guided Domain Adaptation"
author: "[GeekyRakshit](../../)"
date: "2022-07-08"
categories: [computervision, deeplearning, generative, 2-minute-papers]
image: "assets/nada.png"
description: In this article, we take a deep dive into how StyleGAN-NADA achieved the task of CLIP-guided domain adaptation and explore how we can use the model itself.
format: 
  html:
    page-layout: full
---

<details>
    <summary>
        <strong>
            Featured on <a href="https://www.youtube.com/@TwoMinutePapers">Two Minute Papers</a>
        </strong>
    </summary>
    <br>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/MO2K0JXAedM?si=SQ8fY8rA9cjTPOmz" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</details>

<iframe src="https://wandb.ai/geekyrakshit/stylegan-nada/reports/Digging-Into-StyleGAN-NADA-for-CLIP-Guided-Domain-Adaptation--VmlldzoyMjA5MDU1" style="border:none;height:1024px;width:100%"></iframe>