<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="GeekyRakshit">
<meta name="dcterms.date" content="2020-05-05">
<meta name="description" content="Pytorch Implementation of ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation trained on the CamVid Dataset">

<title>Geekyrakshit - Enet-Camvid</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../assets/soumik_rakshit.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="Geekyrakshit - Enet-Camvid">
<meta property="og:description" content="Pytorch Implementation of ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation trained on the CamVid Dataset">
<meta property="og:image" content="https://geekyrakshit.dev/posts/assets/enet-camvid.png">
<meta property="og:site-name" content="Geekyrakshit">
<meta property="og:image:height" content="899">
<meta property="og:image:width" content="900">
<meta name="twitter:title" content="Geekyrakshit - Enet-Camvid">
<meta name="twitter:description" content="Pytorch Implementation of ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation trained on the CamVid Dataset">
<meta name="twitter:image" content="https://geekyrakshit.dev/posts/assets/enet-camvid.png">
<meta name="twitter:image-height" content="899">
<meta name="twitter:image-width" content="900">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Geekyrakshit</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html" rel="" target="">
 <span class="menu-text">About Me</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../courses.html" rel="" target="">
 <span class="menu-text">Courses/Seminars</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../wandb_reports.html" rel="" target="">
 <span class="menu-text">WandB Reports</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../posts.html" rel="" target="">
 <span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/soumik12345" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/soumikRakshit96" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://linkedin.com/in/soumikrakshit/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://www.kaggle.com/soumikrakshit" rel="" target="">
 <span class="menu-text"><i class="fa-brands fa-kaggle" aria-label="kaggle"></i></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://steamcommunity.com/id/geekyRakshit/" rel="" target=""><i class="bi bi-steam" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Enet-Camvid</h1>
  <div class="quarto-categories">
    <div class="quarto-category">computervision</div>
    <div class="quarto-category">deeplearning</div>
    <div class="quarto-category">keras</div>
    <div class="quarto-category">python</div>
    <div class="quarto-category">tensorflow</div>
  </div>
  </div>

<div>
  <div class="description">
    Pytorch Implementation of ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation trained on the CamVid Dataset
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p><a href="../">GeekyRakshit</a> </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 5, 2020</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p><strong>Project Repository:</strong> <a href="https://github.com/soumik12345/enet">https://github.com/soumik12345/enet</a></p>
<section id="network-architecture" class="level2">
<h2 class="anchored" data-anchor-id="network-architecture">Network Architecture</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Imports for Model Creation</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> (</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    Module, Conv2d, ReLU, PReLU, Dropout2d, AvgPool2d,</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    Upsample, MaxPool2d, Sequential, MaxUnpool2d,</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    BatchNorm2d, AdaptiveAvgPool2d, ConvTranspose2d</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> SVG</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="activation-module" class="level3">
<h3 class="anchored" data-anchor-id="activation-module">Activation Module</h3>
<p>Inspired by <strong>Swish</strong> Activation Function (Paper), <strong>Mish</strong> is a Self Regularized Non-Monotonic Neural Activation Function. Mish Activation Function can be mathematically represented by the following formula:</p>
<p><span class="math inline">\(f(x) = x * tanh(ln(1 + e^{x}))\)</span></p>
<p>It can also be represented using the <strong>Softplus</strong> Activation Function: <span class="math inline">\(f(x) = x * tanh( \varsigma (x))\)</span></p>
<p>where, <span class="math inline">\(\varsigma (x) = ln(1 + e^{x})\)</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Mish(Module):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> __init(<span class="va">self</span>):</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">input</span> <span class="op">*</span> torch.tanh(F.softplus(<span class="bu">input</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Since Pytorch does not explicitly have any Module for Activation unlike Tensorflow, we can easily implement it. The following Module could be modified to incorporate for any number of activation functions, each of which can be accessed with a string label.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Activation(Module):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, name<span class="op">=</span><span class="st">'relu'</span>):</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.name <span class="op">=</span> name</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> name <span class="op">==</span> <span class="st">'relu'</span>:</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.act <span class="op">=</span> ReLU()</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> name <span class="op">==</span> <span class="st">'prelu'</span>:</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.act <span class="op">=</span> PReLU()</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> name <span class="op">==</span> <span class="st">'mish'</span>:</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.act <span class="op">=</span> Mish()</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.act(<span class="bu">input</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="enet-initial-block" class="level3">
<h3 class="anchored" data-anchor-id="enet-initial-block">Enet Initial Block</h3>
<p>The Initial Block is the first block of the Enet Model. It consists of 2 branches, a convolutional layer (<code>out_channels=13, kernel_size=3, stride=2</code>) which we would call the main branch in our implementation and a MaxPooling layer which is performed with non-overlapping 2x2 windows which is a secondary block. We would perform BatchNormalization and a Non-linear Activation on the concatenation of the two branches. The Input block would have 16 output channels.</p>
<div class="cell" data-outputid="7ec2c389-26a4-40f4-811c-45e8992c07ac" data-execution_count="7">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">#collapse-hide</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>SVG(<span class="st">'enet_initial_block_xc7itf.svg'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<p><img src="enet-camvid_files/figure-html/cell-5-output-1.svg" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> InitialBlock(Module):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, out_channels, bias<span class="op">=</span><span class="va">False</span>, activation<span class="op">=</span><span class="st">'relu'</span>):</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.main_branch <span class="op">=</span> Conv2d(</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>            in_channels, out_channels <span class="op">-</span> <span class="dv">3</span>,</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>            kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>            padding<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span>bias</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.secondary_branch <span class="op">=</span> MaxPool2d(<span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.batch_norm <span class="op">=</span> BatchNorm2d(out_channels)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> Activation(activation)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        main <span class="op">=</span> <span class="va">self</span>.main_branch(x)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        secondary <span class="op">=</span> <span class="va">self</span>.secondary_branch(x)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> torch.cat((main, secondary), <span class="dv">1</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.batch_norm(output)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.activation(output)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="regular-bottleneck-block" class="level3">
<h3 class="anchored" data-anchor-id="regular-bottleneck-block">Regular Bottleneck Block</h3>
<p>In case of the Regular Bottleneck Block which is the most widely used block in the ENet architecture, the secondary block has no operations. The middle convolution blocks are either <code>3x3</code> regular convolutional block or a <code>5x5</code> asymmetric convolutional block. All the convolutional blocks in the main branch have Batchnormalization and respective Activation after them. The main branch is regularized by a Dropout operation.</p>
<div class="cell" data-outputid="c22b1460-d900-40f9-9e9a-4f91debeb33e" data-execution_count="9">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">#collapse-hide</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>SVG(<span class="st">'enet_regular_bottleneck_esc4ir.svg'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<p><img src="enet-camvid_files/figure-html/cell-7-output-1.svg" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RegularBottleneckBlock(Module):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>, channels, internal_ratio<span class="op">=</span><span class="dv">4</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        dilation<span class="op">=</span><span class="dv">1</span>, asymmetric<span class="op">=</span><span class="va">False</span>, dropout_prob<span class="op">=</span><span class="dv">0</span>, bias<span class="op">=</span><span class="va">False</span>, activation<span class="op">=</span><span class="st">'relu'</span>):</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        internal_channels <span class="op">=</span> channels <span class="op">//</span> internal_ratio</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        <span class="co">### Main Branch </span><span class="al">###</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Block 1 Conv 1x1</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.main_conv_block_1 <span class="op">=</span> Sequential(</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>            Conv2d(</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>                channels, internal_channels,</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>                kernel_size<span class="op">=</span><span class="dv">1</span>, stride<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span>bias</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>            ),</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>            BatchNorm2d(internal_channels),</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>            Activation(activation)</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Block 2</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> asymmetric:</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.main_conv_block_2 <span class="op">=</span> Sequential(</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>                Conv2d(</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>                    internal_channels, internal_channels,</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>                    kernel_size<span class="op">=</span>(kernel_size, <span class="dv">1</span>), stride<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>                    padding<span class="op">=</span>(padding, <span class="dv">0</span>), dilation<span class="op">=</span>dilation, bias<span class="op">=</span>bias</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>                ),</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>                BatchNorm2d(internal_channels),</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>                Activation(activation),</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>                Conv2d(</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>                    internal_channels, internal_channels,</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>                    kernel_size<span class="op">=</span>(<span class="dv">1</span>, kernel_size), stride<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>                    padding<span class="op">=</span>(<span class="dv">0</span>, padding), dilation<span class="op">=</span>dilation, bias<span class="op">=</span>bias</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>                ),</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>                BatchNorm2d(internal_channels),</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>                Activation(activation),</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.main_conv_block_2 <span class="op">=</span> Sequential(</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>                Conv2d(</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>                    internal_channels, internal_channels,</span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>                    kernel_size<span class="op">=</span>kernel_size, stride<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>                    padding<span class="op">=</span>padding, dilation<span class="op">=</span>dilation, bias<span class="op">=</span>bias</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>                ),</span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>                BatchNorm2d(internal_channels),</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>                Activation(activation),</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Block 3 Conv 1x1</span></span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.main_conv_block_3 <span class="op">=</span> Sequential(</span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>            Conv2d(</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>                internal_channels, channels,</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>                kernel_size<span class="op">=</span><span class="dv">1</span>, stride<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span>bias</span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a>            ),</span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>            BatchNorm2d(channels),</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>            Activation(activation),</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Dropout Regularization</span></span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> Dropout2d(p<span class="op">=</span>dropout_prob)</span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Activation</span></span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> Activation(activation)</span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>        secondary_branch <span class="op">=</span> x</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a>        main_branch <span class="op">=</span> <span class="va">self</span>.main_conv_block_1(x)</span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>        main_branch <span class="op">=</span> <span class="va">self</span>.main_conv_block_2(main_branch)</span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a>        main_branch <span class="op">=</span> <span class="va">self</span>.main_conv_block_3(main_branch)</span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a>        main_branch <span class="op">=</span> <span class="va">self</span>.dropout(main_branch)</span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> main_branch <span class="op">+</span> secondary_branch</span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.activation(output)</span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="downsample-bottleneck-block" class="level3">
<h3 class="anchored" data-anchor-id="downsample-bottleneck-block">Downsample Bottleneck Block</h3>
<p>This block is used in the Encoder stages of the Enet Architecture. In this block, the main branch is <code>Conv 1x1 -&gt; Conv 3x3 -&gt; Conv 1x1 -&gt; Dropout</code>. The secondary branch consists of a Maxpooling operation performed with non-overlapping 2x2 windows.</p>
<div class="cell" data-outputid="0e4c4640-69a2-4bc7-ccce-9f63264bb954" data-execution_count="11">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co">#collapse-hide</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>SVG(<span class="st">'enet_downsampling_bottleneck-1_ysayci.svg'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<p><img src="enet-camvid_files/figure-html/cell-9-output-1.svg" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DownsampleBottleneckBlock(Module):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>, in_channels, out_channels, internal_ratio<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>        return_indices<span class="op">=</span><span class="va">False</span>, dropout_prob<span class="op">=</span><span class="dv">0</span>, bias<span class="op">=</span><span class="va">False</span>, activation<span class="op">=</span><span class="st">'relu'</span>):</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        internal_channels <span class="op">=</span> in_channels <span class="op">//</span> internal_ratio</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.return_indices <span class="op">=</span> return_indices</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>        <span class="co">### Main Branch </span><span class="al">###</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Block 1 Conv 1x1</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.main_conv_block_1 <span class="op">=</span> Sequential(</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>            Conv2d(</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>                in_channels, internal_channels,</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>                kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>, bias<span class="op">=</span>bias</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>            ),</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>            BatchNorm2d(internal_channels),</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>            Activation(activation)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Block 2 Conv 3x3</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.main_conv_block_2 <span class="op">=</span> Sequential(</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>            Conv2d(</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>                internal_channels, internal_channels,</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>                kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span>bias</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>            ),</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>            BatchNorm2d(internal_channels),</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>            Activation(activation)</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Block 2 Conv 1x1</span></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.main_conv_block_3 <span class="op">=</span> Sequential(</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>            Conv2d(</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>                internal_channels, out_channels,</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>                kernel_size<span class="op">=</span><span class="dv">1</span>, stride<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span>bias</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>            ),</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>            BatchNorm2d(out_channels),</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>            Activation(activation)</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>        <span class="co">### Secondary Branch </span><span class="al">###</span></span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.secondary_maxpool <span class="op">=</span> MaxPool2d(</span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>            <span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>            return_indices<span class="op">=</span>return_indices</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Dropout Regularization</span></span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> Dropout2d(p<span class="op">=</span>dropout_prob)</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Activation</span></span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> Activation(activation)</span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Main Branch</span></span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a>        main_branch <span class="op">=</span> <span class="va">self</span>.main_conv_block_1(x)</span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a>        main_branch <span class="op">=</span> <span class="va">self</span>.main_conv_block_2(main_branch)</span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a>        main_branch <span class="op">=</span> <span class="va">self</span>.main_conv_block_3(main_branch)</span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Secondary Branch</span></span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.return_indices:</span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a>            secondary_branch, max_indices <span class="op">=</span> <span class="va">self</span>.secondary_maxpool(x)</span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a>            secondary_branch <span class="op">=</span> <span class="va">self</span>.secondary_maxpool(x)</span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Padding</span></span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a>        n, ch_main, h, w <span class="op">=</span> main_branch.size()</span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a>        ch_sec <span class="op">=</span> secondary_branch.size()[<span class="dv">1</span>]</span>
<span id="cb9-69"><a href="#cb9-69" aria-hidden="true" tabindex="-1"></a>        padding <span class="op">=</span> torch.zeros(n, ch_main <span class="op">-</span> ch_sec, h, w)</span>
<span id="cb9-70"><a href="#cb9-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> secondary_branch.is_cuda:</span>
<span id="cb9-71"><a href="#cb9-71" aria-hidden="true" tabindex="-1"></a>            padding <span class="op">=</span> padding.cuda()</span>
<span id="cb9-72"><a href="#cb9-72" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Concatenate</span></span>
<span id="cb9-73"><a href="#cb9-73" aria-hidden="true" tabindex="-1"></a>        secondary_branch <span class="op">=</span> torch.cat((secondary_branch, padding), <span class="dv">1</span>)</span>
<span id="cb9-74"><a href="#cb9-74" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> secondary_branch <span class="op">+</span> main_branch</span>
<span id="cb9-75"><a href="#cb9-75" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.activation(output)</span>
<span id="cb9-76"><a href="#cb9-76" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.return_indices:</span>
<span id="cb9-77"><a href="#cb9-77" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> output, max_indices</span>
<span id="cb9-78"><a href="#cb9-78" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb9-79"><a href="#cb9-79" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="upsampling-bottleneck-block" class="level3">
<h3 class="anchored" data-anchor-id="upsampling-bottleneck-block">Upsampling Bottleneck Block</h3>
<p>This block is used in the Decoder stages of the Enet Architecture. In this block, the main branch is <code>Conv 1x1 -&gt; ConvTranspose2d 3x3 -&gt; Conv 1x1 -&gt; Dropout</code>. The Secondary branch consists of <code>Conv 1x1</code> block followed by a <code>MaxUnpooling2d</code> (Inverse of MaxPooling2d) Block.</p>
<div class="cell" data-outputid="b925eb09-53b1-465b-fc36-8b8c66186913" data-execution_count="13">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">#collapse-hide</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>SVG(<span class="st">'enet_upsampling_block_du1lmw.svg'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<p><img src="enet-camvid_files/figure-html/cell-11-output-1.svg" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> UpsampleBottleneckBlock(Module):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>, in_channels, out_channels,</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>        internal_ratio<span class="op">=</span><span class="dv">4</span>, dropout_prob<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>        bias<span class="op">=</span><span class="va">False</span>, activation<span class="op">=</span><span class="st">'relu'</span>):</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>        internal_channels <span class="op">=</span> in_channels <span class="op">//</span> internal_ratio</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>        <span class="co">### Main Branch </span><span class="al">###</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Block 1 Conv 1x1</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.main_branch_conv_1 <span class="op">=</span> Sequential(</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>            Conv2d(</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>                in_channels, internal_channels,</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>                kernel_size<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span>bias</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>            ),</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>            BatchNorm2d(internal_channels),</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>            Activation(activation)</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Block 2 Transposed Convolution</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.main_branch_transpose_conv_2 <span class="op">=</span> ConvTranspose2d(</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>            internal_channels, internal_channels,</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>            kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>, bias<span class="op">=</span>bias</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.main_branch_bn_2 <span class="op">=</span> BatchNorm2d(internal_channels)</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.main_branch_act_2 <span class="op">=</span> Activation(activation)</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Block 3 Conv 1x1</span></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.main_branch_conv_3 <span class="op">=</span> Sequential(</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>            Conv2d(</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>                internal_channels, out_channels,</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>                kernel_size<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span>bias</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>            ),</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>            BatchNorm2d(out_channels),</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>            Activation(activation)</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>        <span class="co">### Secondary Branch </span><span class="al">###</span></span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.secondary_conv <span class="op">=</span> Sequential(</span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>            Conv2d(</span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>                in_channels, out_channels,</span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a>                kernel_size<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span>bias</span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a>            ),</span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a>            BatchNorm2d(out_channels)</span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.secondary_unpool <span class="op">=</span> MaxUnpool2d(kernel_size<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Dropout Regularization</span></span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> Dropout2d(p<span class="op">=</span>dropout_prob)</span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Activation</span></span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> Activation(activation)</span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, max_indices, output_size):</span>
<span id="cb11-59"><a href="#cb11-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Main Branch</span></span>
<span id="cb11-60"><a href="#cb11-60" aria-hidden="true" tabindex="-1"></a>        main_branch <span class="op">=</span> <span class="va">self</span>.main_branch_conv_1(x)</span>
<span id="cb11-61"><a href="#cb11-61" aria-hidden="true" tabindex="-1"></a>        main_branch <span class="op">=</span> <span class="va">self</span>.main_branch_transpose_conv_2(main_branch, output_size<span class="op">=</span>output_size)</span>
<span id="cb11-62"><a href="#cb11-62" aria-hidden="true" tabindex="-1"></a>        main_branch <span class="op">=</span> <span class="va">self</span>.main_branch_bn_2(main_branch)</span>
<span id="cb11-63"><a href="#cb11-63" aria-hidden="true" tabindex="-1"></a>        main_branch <span class="op">=</span> <span class="va">self</span>.main_branch_act_2(main_branch)</span>
<span id="cb11-64"><a href="#cb11-64" aria-hidden="true" tabindex="-1"></a>        main_branch <span class="op">=</span> <span class="va">self</span>.main_branch_conv_3(main_branch)</span>
<span id="cb11-65"><a href="#cb11-65" aria-hidden="true" tabindex="-1"></a>        main_branch <span class="op">=</span> <span class="va">self</span>.dropout(main_branch)</span>
<span id="cb11-66"><a href="#cb11-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Secondary Branch</span></span>
<span id="cb11-67"><a href="#cb11-67" aria-hidden="true" tabindex="-1"></a>        secondary_branch <span class="op">=</span> <span class="va">self</span>.secondary_conv(x)</span>
<span id="cb11-68"><a href="#cb11-68" aria-hidden="true" tabindex="-1"></a>        secondary_branch <span class="op">=</span> <span class="va">self</span>.secondary_unpool(</span>
<span id="cb11-69"><a href="#cb11-69" aria-hidden="true" tabindex="-1"></a>            secondary_branch, max_indices,</span>
<span id="cb11-70"><a href="#cb11-70" aria-hidden="true" tabindex="-1"></a>            output_size<span class="op">=</span>output_size</span>
<span id="cb11-71"><a href="#cb11-71" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb11-72"><a href="#cb11-72" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Concatenate</span></span>
<span id="cb11-73"><a href="#cb11-73" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> main_branch <span class="op">+</span> secondary_branch</span>
<span id="cb11-74"><a href="#cb11-74" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.activation(output)</span>
<span id="cb11-75"><a href="#cb11-75" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="building-enet" class="level3">
<h3 class="anchored" data-anchor-id="building-enet">Building Enet</h3>
<p>The overall architecture of Enet is summarized in the following table. The whole architecture is divided into 6 parts or stages. Stage <code>0</code> consists of the Initial Block only. Stages <code>1-3</code> make up the encoder part of the network which downsamples the input. Stages <code>4-5</code> makes up the decoder, which upsamples the input to create the output.</p>
<table class="table">
<thead>
<tr class="header">
<th>Name</th>
<th>Type</th>
<th>Output Size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Initial</td>
<td></td>
<td><code>16x256x256</code></td>
</tr>
<tr class="even">
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="odd">
<td>Bottleneck_1</td>
<td>Downsampling</td>
<td><code>64x128x128</code></td>
</tr>
<tr class="even">
<td>RegularBottleneck_1_1</td>
<td></td>
<td><code>64x128x128</code></td>
</tr>
<tr class="odd">
<td>RegularBottleneck_1_2</td>
<td></td>
<td><code>64x128x128</code></td>
</tr>
<tr class="even">
<td>RegularBottleneck_1_3</td>
<td></td>
<td><code>64x128x128</code></td>
</tr>
<tr class="odd">
<td>RegularBottleneck_1_4</td>
<td></td>
<td><code>64x128x128</code></td>
</tr>
<tr class="even">
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="odd">
<td>Bottleneck_2</td>
<td>Downsampling</td>
<td><code>128x64x64</code></td>
</tr>
<tr class="even">
<td>RegularBottleneck_2_1</td>
<td></td>
<td><code>128x64x64</code></td>
</tr>
<tr class="odd">
<td>RegularBottleneck_2_2</td>
<td>Dilated 2</td>
<td><code>128x64x64</code></td>
</tr>
<tr class="even">
<td>RegularBottleneck_2_3</td>
<td>Asymmetric 5</td>
<td><code>128x64x64</code></td>
</tr>
<tr class="odd">
<td>RegularBottleneck_2_4</td>
<td>Dilated 4</td>
<td><code>128x64x64</code></td>
</tr>
<tr class="even">
<td>RegularBottleneck_2_5</td>
<td></td>
<td><code>128x64x64</code></td>
</tr>
<tr class="odd">
<td>RegularBottleneck_2_6</td>
<td>Dilated 8</td>
<td><code>128x64x64</code></td>
</tr>
<tr class="even">
<td>RegularBottleneck_2_7</td>
<td>Asymmetric 5</td>
<td><code>128x64x64</code></td>
</tr>
<tr class="odd">
<td>RegularBottleneck_2_8</td>
<td>Dilated 16</td>
<td><code>128x64x64</code></td>
</tr>
<tr class="even">
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="odd">
<td>RegularBottleneck_3</td>
<td></td>
<td><code>128x64x64</code></td>
</tr>
<tr class="even">
<td>RegularBottleneck_3_1</td>
<td>Dilated 2</td>
<td><code>128x64x64</code></td>
</tr>
<tr class="odd">
<td>RegularBottleneck_3_2</td>
<td>Assymetric 5</td>
<td><code>128x64x64</code></td>
</tr>
<tr class="even">
<td>RegularBottleneck_3_3</td>
<td>Dilated 4</td>
<td><code>128x64x64</code></td>
</tr>
<tr class="odd">
<td>RegularBottleneck_3_4</td>
<td></td>
<td><code>128x64x64</code></td>
</tr>
<tr class="even">
<td>RegularBottleneck_3_5</td>
<td>Dilated 8</td>
<td><code>128x64x64</code></td>
</tr>
<tr class="odd">
<td>RegularBottleneck_3_6</td>
<td>Asymmetric 5</td>
<td><code>128x64x64</code></td>
</tr>
<tr class="even">
<td>RegularBottleneck_3_7</td>
<td>Dilated 16</td>
<td><code>128x64x64</code></td>
</tr>
<tr class="odd">
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="even">
<td>Bottleneck_4</td>
<td>Upsampling</td>
<td><code>64x128x128</code></td>
</tr>
<tr class="odd">
<td>Bottleneck_4_1</td>
<td></td>
<td><code>64x128x128</code></td>
</tr>
<tr class="even">
<td>Bottleneck_4_2</td>
<td></td>
<td><code>64x128x128</code></td>
</tr>
<tr class="odd">
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="even">
<td>Bottleneck_5</td>
<td>Upsampling</td>
<td><code>16x256x256</code></td>
</tr>
<tr class="odd">
<td>Bottleneck_5_1</td>
<td></td>
<td><code>16x256x256</code></td>
</tr>
<tr class="even">
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="odd">
<td>Transposed_Conv</td>
<td></td>
<td><code>Cx512x512</code></td>
</tr>
</tbody>
</table>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Enet(Module):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_classes, encoder_activation<span class="op">=</span><span class="st">'mish'</span>, decoder_activation<span class="op">=</span><span class="st">'relu'</span>):</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initial Block</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.initial_block <span class="op">=</span> InitialBlock(<span class="dv">3</span>, <span class="dv">16</span>, activation<span class="op">=</span>encoder_activation)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>        <span class="co">### Encoding Stages </span><span class="al">###</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Stage 1</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.down_bottleneck_1 <span class="op">=</span> DownsampleBottleneckBlock(</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>            <span class="dv">16</span>, <span class="dv">64</span>, return_indices<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>            dropout_prob<span class="op">=</span><span class="fl">0.01</span>, activation<span class="op">=</span>encoder_activation</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bottleneck_1_1 <span class="op">=</span> RegularBottleneckBlock(</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>            <span class="dv">64</span>, padding<span class="op">=</span><span class="dv">1</span>, dropout_prob<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>            activation<span class="op">=</span>encoder_activation</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bottleneck_1_2 <span class="op">=</span> RegularBottleneckBlock(</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>            <span class="dv">64</span>, padding<span class="op">=</span><span class="dv">1</span>, dropout_prob<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>            activation<span class="op">=</span>encoder_activation</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bottleneck_1_3 <span class="op">=</span> RegularBottleneckBlock(</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>            <span class="dv">64</span>, padding<span class="op">=</span><span class="dv">1</span>, dropout_prob<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>            activation<span class="op">=</span>encoder_activation</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bottleneck_1_4 <span class="op">=</span> RegularBottleneckBlock(</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>            <span class="dv">64</span>, padding<span class="op">=</span><span class="dv">1</span>, dropout_prob<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>            activation<span class="op">=</span>encoder_activation</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Stage 2</span></span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.down_bottleneck_2 <span class="op">=</span> DownsampleBottleneckBlock(</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>            <span class="dv">64</span>, <span class="dv">128</span>, return_indices<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>            dropout_prob<span class="op">=</span><span class="fl">0.1</span>, activation<span class="op">=</span>encoder_activation</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bottleneck_2_1 <span class="op">=</span> RegularBottleneckBlock(</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>            <span class="dv">128</span>, padding<span class="op">=</span><span class="dv">1</span>, dropout_prob<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>            activation<span class="op">=</span>encoder_activation</span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bottleneck_2_2 <span class="op">=</span> RegularBottleneckBlock(</span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>            <span class="dv">128</span>, dilation<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>            padding<span class="op">=</span><span class="dv">2</span>, dropout_prob<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>            activation<span class="op">=</span>encoder_activation</span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bottleneck_2_3 <span class="op">=</span> RegularBottleneckBlock(</span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>            <span class="dv">128</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a>            asymmetric<span class="op">=</span><span class="va">True</span>, dropout_prob<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a>            activation<span class="op">=</span>encoder_activation</span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bottleneck_2_4 <span class="op">=</span> RegularBottleneckBlock(</span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a>            <span class="dv">128</span>, dilation<span class="op">=</span><span class="dv">4</span>, padding<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a>            dropout_prob<span class="op">=</span><span class="fl">0.1</span>, activation<span class="op">=</span>encoder_activation</span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bottleneck_2_5 <span class="op">=</span> RegularBottleneckBlock(</span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a>            <span class="dv">128</span>, padding<span class="op">=</span><span class="dv">1</span>, dropout_prob<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a>            activation<span class="op">=</span>encoder_activation</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bottleneck_2_6 <span class="op">=</span> RegularBottleneckBlock(</span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a>            <span class="dv">128</span>, dilation<span class="op">=</span><span class="dv">8</span>, padding<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a>            dropout_prob<span class="op">=</span><span class="fl">0.1</span>, activation<span class="op">=</span>encoder_activation</span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bottleneck_2_7 <span class="op">=</span> RegularBottleneckBlock(</span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a>            <span class="dv">128</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, asymmetric<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a>            padding<span class="op">=</span><span class="dv">2</span>, dropout_prob<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a>            activation<span class="op">=</span>encoder_activation</span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bottleneck_2_8 <span class="op">=</span> RegularBottleneckBlock(</span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a>            <span class="dv">128</span>, dilation<span class="op">=</span><span class="dv">16</span>, padding<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a>            dropout_prob<span class="op">=</span><span class="fl">0.1</span>, activation<span class="op">=</span>encoder_activation</span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Stage 3</span></span>
<span id="cb12-76"><a href="#cb12-76" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.regular_bottleneck_3 <span class="op">=</span> RegularBottleneckBlock(</span>
<span id="cb12-77"><a href="#cb12-77" aria-hidden="true" tabindex="-1"></a>            <span class="dv">128</span>, padding<span class="op">=</span><span class="dv">1</span>, dropout_prob<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb12-78"><a href="#cb12-78" aria-hidden="true" tabindex="-1"></a>            activation<span class="op">=</span>encoder_activation</span>
<span id="cb12-79"><a href="#cb12-79" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-80"><a href="#cb12-80" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bottleneck_3_1 <span class="op">=</span> RegularBottleneckBlock(</span>
<span id="cb12-81"><a href="#cb12-81" aria-hidden="true" tabindex="-1"></a>            <span class="dv">128</span>, dilation<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb12-82"><a href="#cb12-82" aria-hidden="true" tabindex="-1"></a>            dropout_prob<span class="op">=</span><span class="fl">0.1</span>, activation<span class="op">=</span>encoder_activation</span>
<span id="cb12-83"><a href="#cb12-83" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-84"><a href="#cb12-84" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bottleneck_3_2 <span class="op">=</span> RegularBottleneckBlock(</span>
<span id="cb12-85"><a href="#cb12-85" aria-hidden="true" tabindex="-1"></a>            <span class="dv">128</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb12-86"><a href="#cb12-86" aria-hidden="true" tabindex="-1"></a>            asymmetric<span class="op">=</span><span class="va">True</span>, dropout_prob<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb12-87"><a href="#cb12-87" aria-hidden="true" tabindex="-1"></a>            activation<span class="op">=</span>encoder_activation</span>
<span id="cb12-88"><a href="#cb12-88" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-89"><a href="#cb12-89" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bottleneck_3_3 <span class="op">=</span> RegularBottleneckBlock(</span>
<span id="cb12-90"><a href="#cb12-90" aria-hidden="true" tabindex="-1"></a>            <span class="dv">128</span>, dilation<span class="op">=</span><span class="dv">4</span>, padding<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb12-91"><a href="#cb12-91" aria-hidden="true" tabindex="-1"></a>            dropout_prob<span class="op">=</span><span class="fl">0.1</span>, activation<span class="op">=</span>encoder_activation</span>
<span id="cb12-92"><a href="#cb12-92" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-93"><a href="#cb12-93" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bottleneck_3_4 <span class="op">=</span> RegularBottleneckBlock(</span>
<span id="cb12-94"><a href="#cb12-94" aria-hidden="true" tabindex="-1"></a>            <span class="dv">128</span>, padding<span class="op">=</span><span class="dv">1</span>, dropout_prob<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb12-95"><a href="#cb12-95" aria-hidden="true" tabindex="-1"></a>            activation<span class="op">=</span>encoder_activation</span>
<span id="cb12-96"><a href="#cb12-96" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-97"><a href="#cb12-97" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bottleneck_3_5 <span class="op">=</span> RegularBottleneckBlock(</span>
<span id="cb12-98"><a href="#cb12-98" aria-hidden="true" tabindex="-1"></a>            <span class="dv">128</span>, dilation<span class="op">=</span><span class="dv">8</span>, padding<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb12-99"><a href="#cb12-99" aria-hidden="true" tabindex="-1"></a>            dropout_prob<span class="op">=</span><span class="fl">0.1</span>, activation<span class="op">=</span>encoder_activation</span>
<span id="cb12-100"><a href="#cb12-100" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-101"><a href="#cb12-101" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bottleneck_3_6 <span class="op">=</span> RegularBottleneckBlock(</span>
<span id="cb12-102"><a href="#cb12-102" aria-hidden="true" tabindex="-1"></a>            <span class="dv">128</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, asymmetric<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-103"><a href="#cb12-103" aria-hidden="true" tabindex="-1"></a>            padding<span class="op">=</span><span class="dv">2</span>, dropout_prob<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb12-104"><a href="#cb12-104" aria-hidden="true" tabindex="-1"></a>            activation<span class="op">=</span>encoder_activation</span>
<span id="cb12-105"><a href="#cb12-105" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-106"><a href="#cb12-106" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bottleneck_3_7 <span class="op">=</span> RegularBottleneckBlock(</span>
<span id="cb12-107"><a href="#cb12-107" aria-hidden="true" tabindex="-1"></a>            <span class="dv">128</span>, dilation<span class="op">=</span><span class="dv">16</span>, padding<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb12-108"><a href="#cb12-108" aria-hidden="true" tabindex="-1"></a>            dropout_prob<span class="op">=</span><span class="fl">0.1</span>, activation<span class="op">=</span>encoder_activation</span>
<span id="cb12-109"><a href="#cb12-109" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-110"><a href="#cb12-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-111"><a href="#cb12-111" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Stage 4</span></span>
<span id="cb12-112"><a href="#cb12-112" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.upsample_4 <span class="op">=</span> UpsampleBottleneckBlock(</span>
<span id="cb12-113"><a href="#cb12-113" aria-hidden="true" tabindex="-1"></a>            <span class="dv">128</span>, <span class="dv">64</span>, dropout_prob<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb12-114"><a href="#cb12-114" aria-hidden="true" tabindex="-1"></a>            activation<span class="op">=</span>decoder_activation</span>
<span id="cb12-115"><a href="#cb12-115" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-116"><a href="#cb12-116" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bottleneck_4_1 <span class="op">=</span> RegularBottleneckBlock(</span>
<span id="cb12-117"><a href="#cb12-117" aria-hidden="true" tabindex="-1"></a>            <span class="dv">64</span>, padding<span class="op">=</span><span class="dv">1</span>, dropout_prob<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb12-118"><a href="#cb12-118" aria-hidden="true" tabindex="-1"></a>            activation<span class="op">=</span>decoder_activation</span>
<span id="cb12-119"><a href="#cb12-119" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-120"><a href="#cb12-120" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bottleneck_4_2 <span class="op">=</span> RegularBottleneckBlock(</span>
<span id="cb12-121"><a href="#cb12-121" aria-hidden="true" tabindex="-1"></a>            <span class="dv">64</span>, padding<span class="op">=</span><span class="dv">1</span>, dropout_prob<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb12-122"><a href="#cb12-122" aria-hidden="true" tabindex="-1"></a>            activation<span class="op">=</span>decoder_activation</span>
<span id="cb12-123"><a href="#cb12-123" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-124"><a href="#cb12-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-125"><a href="#cb12-125" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Stage 5</span></span>
<span id="cb12-126"><a href="#cb12-126" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.upsample_5 <span class="op">=</span> UpsampleBottleneckBlock(</span>
<span id="cb12-127"><a href="#cb12-127" aria-hidden="true" tabindex="-1"></a>            <span class="dv">64</span>, <span class="dv">16</span>, dropout_prob<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb12-128"><a href="#cb12-128" aria-hidden="true" tabindex="-1"></a>            activation<span class="op">=</span>decoder_activation</span>
<span id="cb12-129"><a href="#cb12-129" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-130"><a href="#cb12-130" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bottleneck_5 <span class="op">=</span> RegularBottleneckBlock(</span>
<span id="cb12-131"><a href="#cb12-131" aria-hidden="true" tabindex="-1"></a>            <span class="dv">16</span>, padding<span class="op">=</span><span class="dv">1</span>, dropout_prob<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb12-132"><a href="#cb12-132" aria-hidden="true" tabindex="-1"></a>            activation<span class="op">=</span>decoder_activation</span>
<span id="cb12-133"><a href="#cb12-133" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-134"><a href="#cb12-134" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transposed_conv <span class="op">=</span> ConvTranspose2d(</span>
<span id="cb12-135"><a href="#cb12-135" aria-hidden="true" tabindex="-1"></a>            <span class="dv">16</span>, num_classes, kernel_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb12-136"><a href="#cb12-136" aria-hidden="true" tabindex="-1"></a>            stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>, output_padding<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span></span>
<span id="cb12-137"><a href="#cb12-137" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-138"><a href="#cb12-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-139"><a href="#cb12-139" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb12-140"><a href="#cb12-140" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initial Block</span></span>
<span id="cb12-141"><a href="#cb12-141" aria-hidden="true" tabindex="-1"></a>        input_size <span class="op">=</span> x.size()</span>
<span id="cb12-142"><a href="#cb12-142" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.initial_block(x)</span>
<span id="cb12-143"><a href="#cb12-143" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Stage 1</span></span>
<span id="cb12-144"><a href="#cb12-144" aria-hidden="true" tabindex="-1"></a>        input_size_1 <span class="op">=</span> x.size()</span>
<span id="cb12-145"><a href="#cb12-145" aria-hidden="true" tabindex="-1"></a>        x, max_indices_1 <span class="op">=</span> <span class="va">self</span>.down_bottleneck_1(x)</span>
<span id="cb12-146"><a href="#cb12-146" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bottleneck_1_1(x)</span>
<span id="cb12-147"><a href="#cb12-147" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bottleneck_1_2(x)</span>
<span id="cb12-148"><a href="#cb12-148" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bottleneck_1_3(x)</span>
<span id="cb12-149"><a href="#cb12-149" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bottleneck_1_4(x)</span>
<span id="cb12-150"><a href="#cb12-150" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Stage 2</span></span>
<span id="cb12-151"><a href="#cb12-151" aria-hidden="true" tabindex="-1"></a>        input_size_2 <span class="op">=</span> x.size()</span>
<span id="cb12-152"><a href="#cb12-152" aria-hidden="true" tabindex="-1"></a>        x, max_indices_2 <span class="op">=</span> <span class="va">self</span>.down_bottleneck_2(x)</span>
<span id="cb12-153"><a href="#cb12-153" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bottleneck_2_1(x)</span>
<span id="cb12-154"><a href="#cb12-154" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bottleneck_2_2(x)</span>
<span id="cb12-155"><a href="#cb12-155" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bottleneck_2_3(x)</span>
<span id="cb12-156"><a href="#cb12-156" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bottleneck_2_4(x)</span>
<span id="cb12-157"><a href="#cb12-157" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bottleneck_2_5(x)</span>
<span id="cb12-158"><a href="#cb12-158" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bottleneck_2_6(x)</span>
<span id="cb12-159"><a href="#cb12-159" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bottleneck_2_7(x)</span>
<span id="cb12-160"><a href="#cb12-160" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bottleneck_2_8(x)</span>
<span id="cb12-161"><a href="#cb12-161" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Stage 3</span></span>
<span id="cb12-162"><a href="#cb12-162" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.regular_bottleneck_3(x)</span>
<span id="cb12-163"><a href="#cb12-163" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bottleneck_3_1(x)</span>
<span id="cb12-164"><a href="#cb12-164" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bottleneck_3_2(x)</span>
<span id="cb12-165"><a href="#cb12-165" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bottleneck_3_3(x)</span>
<span id="cb12-166"><a href="#cb12-166" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bottleneck_3_4(x)</span>
<span id="cb12-167"><a href="#cb12-167" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bottleneck_3_5(x)</span>
<span id="cb12-168"><a href="#cb12-168" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bottleneck_3_6(x)</span>
<span id="cb12-169"><a href="#cb12-169" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bottleneck_3_7(x)</span>
<span id="cb12-170"><a href="#cb12-170" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Stage 4</span></span>
<span id="cb12-171"><a href="#cb12-171" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.upsample_4(x, max_indices_2, output_size<span class="op">=</span>input_size_2)</span>
<span id="cb12-172"><a href="#cb12-172" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bottleneck_4_1(x)</span>
<span id="cb12-173"><a href="#cb12-173" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bottleneck_4_2(x)</span>
<span id="cb12-174"><a href="#cb12-174" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Stage 5</span></span>
<span id="cb12-175"><a href="#cb12-175" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.upsample_5(x, max_indices_1, output_size<span class="op">=</span>input_size_1)</span>
<span id="cb12-176"><a href="#cb12-176" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bottleneck_5(x)</span>
<span id="cb12-177"><a href="#cb12-177" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.transposed_conv(x)</span>
<span id="cb12-178"><a href="#cb12-178" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="c85120a6-1302-4d0b-e585-05273309f146">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'GPU:'</span>, torch.cuda.get_device_name(<span class="dv">0</span>))</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">'cuda:0'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>enet <span class="op">=</span> Enet(<span class="dv">12</span>, encoder_activation<span class="op">=</span><span class="st">'prelu'</span>, decoder_activation<span class="op">=</span><span class="st">'relu'</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>enet <span class="op">=</span> enet.to(device)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(enet)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>GPU: Quadro P5000
Enet(
  (initial_block): InitialBlock(
    (main_branch): Conv2d(3, 13, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (secondary_branch): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activation): Activation(
      (act): PReLU(num_parameters=1)
    )
  )
  (down_bottleneck_1): DownsampleBottleneckBlock(
    (main_conv_block_1): Sequential(
      (0): Conv2d(16, 4, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_2): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_3): Sequential(
      (0): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (secondary_maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (dropout): Dropout2d(p=0.01, inplace=False)
    (activation): Activation(
      (act): PReLU(num_parameters=1)
    )
  )
  (bottleneck_1_1): RegularBottleneckBlock(
    (main_conv_block_1): Sequential(
      (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_2): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_3): Sequential(
      (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (dropout): Dropout2d(p=0.01, inplace=False)
    (activation): Activation(
      (act): PReLU(num_parameters=1)
    )
  )
  (bottleneck_1_2): RegularBottleneckBlock(
    (main_conv_block_1): Sequential(
      (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_2): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_3): Sequential(
      (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (dropout): Dropout2d(p=0.01, inplace=False)
    (activation): Activation(
      (act): PReLU(num_parameters=1)
    )
  )
  (bottleneck_1_3): RegularBottleneckBlock(
    (main_conv_block_1): Sequential(
      (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_2): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_3): Sequential(
      (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (dropout): Dropout2d(p=0.01, inplace=False)
    (activation): Activation(
      (act): PReLU(num_parameters=1)
    )
  )
  (bottleneck_1_4): RegularBottleneckBlock(
    (main_conv_block_1): Sequential(
      (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_2): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_3): Sequential(
      (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (dropout): Dropout2d(p=0.01, inplace=False)
    (activation): Activation(
      (act): PReLU(num_parameters=1)
    )
  )
  (down_bottleneck_2): DownsampleBottleneckBlock(
    (main_conv_block_1): Sequential(
      (0): Conv2d(64, 16, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_2): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_3): Sequential(
      (0): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (secondary_maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (dropout): Dropout2d(p=0.1, inplace=False)
    (activation): Activation(
      (act): PReLU(num_parameters=1)
    )
  )
  (bottleneck_2_1): RegularBottleneckBlock(
    (main_conv_block_1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (dropout): Dropout2d(p=0.1, inplace=False)
    (activation): Activation(
      (act): PReLU(num_parameters=1)
    )
  )
  (bottleneck_2_2): RegularBottleneckBlock(
    (main_conv_block_1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (dropout): Dropout2d(p=0.1, inplace=False)
    (activation): Activation(
      (act): PReLU(num_parameters=1)
    )
  )
  (bottleneck_2_3): RegularBottleneckBlock(
    (main_conv_block_1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
      (3): Conv2d(32, 32, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (dropout): Dropout2d(p=0.1, inplace=False)
    (activation): Activation(
      (act): PReLU(num_parameters=1)
    )
  )
  (bottleneck_2_4): RegularBottleneckBlock(
    (main_conv_block_1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (dropout): Dropout2d(p=0.1, inplace=False)
    (activation): Activation(
      (act): PReLU(num_parameters=1)
    )
  )
  (bottleneck_2_5): RegularBottleneckBlock(
    (main_conv_block_1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (dropout): Dropout2d(p=0.1, inplace=False)
    (activation): Activation(
      (act): PReLU(num_parameters=1)
    )
  )
  (bottleneck_2_6): RegularBottleneckBlock(
    (main_conv_block_1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (dropout): Dropout2d(p=0.1, inplace=False)
    (activation): Activation(
      (act): PReLU(num_parameters=1)
    )
  )
  (bottleneck_2_7): RegularBottleneckBlock(
    (main_conv_block_1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
      (3): Conv2d(32, 32, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (dropout): Dropout2d(p=0.1, inplace=False)
    (activation): Activation(
      (act): PReLU(num_parameters=1)
    )
  )
  (bottleneck_2_8): RegularBottleneckBlock(
    (main_conv_block_1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(16, 16), dilation=(16, 16), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (dropout): Dropout2d(p=0.1, inplace=False)
    (activation): Activation(
      (act): PReLU(num_parameters=1)
    )
  )
  (regular_bottleneck_3): RegularBottleneckBlock(
    (main_conv_block_1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (dropout): Dropout2d(p=0.1, inplace=False)
    (activation): Activation(
      (act): PReLU(num_parameters=1)
    )
  )
  (bottleneck_3_1): RegularBottleneckBlock(
    (main_conv_block_1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (dropout): Dropout2d(p=0.1, inplace=False)
    (activation): Activation(
      (act): PReLU(num_parameters=1)
    )
  )
  (bottleneck_3_2): RegularBottleneckBlock(
    (main_conv_block_1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
      (3): Conv2d(32, 32, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (dropout): Dropout2d(p=0.1, inplace=False)
    (activation): Activation(
      (act): PReLU(num_parameters=1)
    )
  )
  (bottleneck_3_3): RegularBottleneckBlock(
    (main_conv_block_1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (dropout): Dropout2d(p=0.1, inplace=False)
    (activation): Activation(
      (act): PReLU(num_parameters=1)
    )
  )
  (bottleneck_3_4): RegularBottleneckBlock(
    (main_conv_block_1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (dropout): Dropout2d(p=0.1, inplace=False)
    (activation): Activation(
      (act): PReLU(num_parameters=1)
    )
  )
  (bottleneck_3_5): RegularBottleneckBlock(
    (main_conv_block_1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (dropout): Dropout2d(p=0.1, inplace=False)
    (activation): Activation(
      (act): PReLU(num_parameters=1)
    )
  )
  (bottleneck_3_6): RegularBottleneckBlock(
    (main_conv_block_1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
      (3): Conv2d(32, 32, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (dropout): Dropout2d(p=0.1, inplace=False)
    (activation): Activation(
      (act): PReLU(num_parameters=1)
    )
  )
  (bottleneck_3_7): RegularBottleneckBlock(
    (main_conv_block_1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(16, 16), dilation=(16, 16), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (main_conv_block_3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): PReLU(num_parameters=1)
      )
    )
    (dropout): Dropout2d(p=0.1, inplace=False)
    (activation): Activation(
      (act): PReLU(num_parameters=1)
    )
  )
  (upsample_4): UpsampleBottleneckBlock(
    (main_branch_conv_1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): ReLU()
      )
    )
    (main_branch_transpose_conv_2): ConvTranspose2d(32, 32, kernel_size=(2, 2), stride=(2, 2), bias=False)
    (main_branch_bn_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (main_branch_act_2): Activation(
      (act): ReLU()
    )
    (main_branch_conv_3): Sequential(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): ReLU()
      )
    )
    (secondary_conv): Sequential(
      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (secondary_unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))
    (dropout): Dropout2d(p=0.1, inplace=False)
    (activation): Activation(
      (act): ReLU()
    )
  )
  (bottleneck_4_1): RegularBottleneckBlock(
    (main_conv_block_1): Sequential(
      (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): ReLU()
      )
    )
    (main_conv_block_2): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): ReLU()
      )
    )
    (main_conv_block_3): Sequential(
      (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): ReLU()
      )
    )
    (dropout): Dropout2d(p=0.1, inplace=False)
    (activation): Activation(
      (act): ReLU()
    )
  )
  (bottleneck_4_2): RegularBottleneckBlock(
    (main_conv_block_1): Sequential(
      (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): ReLU()
      )
    )
    (main_conv_block_2): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): ReLU()
      )
    )
    (main_conv_block_3): Sequential(
      (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): ReLU()
      )
    )
    (dropout): Dropout2d(p=0.1, inplace=False)
    (activation): Activation(
      (act): ReLU()
    )
  )
  (upsample_5): UpsampleBottleneckBlock(
    (main_branch_conv_1): Sequential(
      (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): ReLU()
      )
    )
    (main_branch_transpose_conv_2): ConvTranspose2d(16, 16, kernel_size=(2, 2), stride=(2, 2), bias=False)
    (main_branch_bn_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (main_branch_act_2): Activation(
      (act): ReLU()
    )
    (main_branch_conv_3): Sequential(
      (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): ReLU()
      )
    )
    (secondary_conv): Sequential(
      (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (secondary_unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))
    (dropout): Dropout2d(p=0.1, inplace=False)
    (activation): Activation(
      (act): ReLU()
    )
  )
  (bottleneck_5): RegularBottleneckBlock(
    (main_conv_block_1): Sequential(
      (0): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): ReLU()
      )
    )
    (main_conv_block_2): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): ReLU()
      )
    )
    (main_conv_block_3): Sequential(
      (0): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Activation(
        (act): ReLU()
      )
    )
    (dropout): Dropout2d(p=0.1, inplace=False)
    (activation): Activation(
      (act): ReLU()
    )
  )
  (transposed_conv): ConvTranspose2d(16, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
)</code></pre>
</div>
</div>
</section>
</section>
<section id="pipeline-for-camvid-dataset" class="level2">
<h2 class="anchored" data-anchor-id="pipeline-for-camvid-dataset">Pipeline for Camvid Dataset</h2>
<section id="download-dataset" class="level3">
<h3 class="anchored" data-anchor-id="download-dataset">Download Dataset</h3>
<div class="cell" data-outputid="ef1749a6-5109-4911-e2bb-c45f77a65f41">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>mkdir camvid</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>cd camvid</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>wget https:<span class="op">//</span>www.dropbox.com<span class="op">/</span>s<span class="op">/</span>ej1gx48bxqbtwd2<span class="op">/</span>CamVid.<span class="bu">zip</span>?dl<span class="op">=</span><span class="dv">0</span> <span class="op">-</span>O CamVid.<span class="bu">zip</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>unzip <span class="op">-</span>qq CamVid.<span class="bu">zip</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>rm CamVid.<span class="bu">zip</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>cd ..</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>/notebooks/camvid
--2019-12-22 17:08:27--  https://www.dropbox.com/s/ej1gx48bxqbtwd2/CamVid.zip?dl=0
Resolving www.dropbox.com (www.dropbox.com)... 162.125.7.1, 2620:100:601a:1::a27d:701
Connecting to www.dropbox.com (www.dropbox.com)|162.125.7.1|:443... connected.
HTTP request sent, awaiting response... 301 Moved Permanently
Location: /s/raw/ej1gx48bxqbtwd2/CamVid.zip [following]
--2019-12-22 17:08:27--  https://www.dropbox.com/s/raw/ej1gx48bxqbtwd2/CamVid.zip
Reusing existing connection to www.dropbox.com:443.
HTTP request sent, awaiting response... 302 Found
Location: https://uc5e0c03b1fd422e4efdf9b56c46.dl.dropboxusercontent.com/cd/0/inline/Auv6VxzcBuE4iS57nb8ME8vAwaN4ktj3ntJn3Sfvu9OVl8uIdY83BB2uBh2hpf2XOGPfSPccgOW5ngRb-3iOMV-CouNNKyCf0wsmWSv5zvU6m3zieRaW9-IfjCJzpJd2ZYY/file# [following]
--2019-12-22 17:08:27--  https://uc5e0c03b1fd422e4efdf9b56c46.dl.dropboxusercontent.com/cd/0/inline/Auv6VxzcBuE4iS57nb8ME8vAwaN4ktj3ntJn3Sfvu9OVl8uIdY83BB2uBh2hpf2XOGPfSPccgOW5ngRb-3iOMV-CouNNKyCf0wsmWSv5zvU6m3zieRaW9-IfjCJzpJd2ZYY/file
Resolving uc5e0c03b1fd422e4efdf9b56c46.dl.dropboxusercontent.com (uc5e0c03b1fd422e4efdf9b56c46.dl.dropboxusercontent.com)... 162.125.7.6, 2620:100:601a:6::a27d:706
Connecting to uc5e0c03b1fd422e4efdf9b56c46.dl.dropboxusercontent.com (uc5e0c03b1fd422e4efdf9b56c46.dl.dropboxusercontent.com)|162.125.7.6|:443... connected.
HTTP request sent, awaiting response... 302 FOUND
Location: /cd/0/inline2/Auv2moV75j6bD1ZVWnQ8TzgrCT8YBavOXNvuI93KzHABa5foU5NHalPz2iQVfVIxJqmlUNryF9fCadSY8v0pFlH56XP593-KMVw74ZrrfGxJx7cylo6nofaqbN6VgrPRTVrHRCmsZN5jvmf_tjdUSF3AbS5eYHVZFyJk7hjbVMW4nOEc2R5qNXUtHfBhgNnPVALcCzygAzHAPG85VZPW-lEbnVPgCybPUM-cNsxBEKOGQJtGIAAelpJyJpppUuIIr3hgXBHKLylw-29RpcKkeADZ3MJHzAIEnXehRGlzhe7aYzWquPr6-Pqi82AWy9sieCEC2m8xAWKZgpWG7Vw5-F1LZpihT2zqET2lfb6JtpP4bg/file [following]
--2019-12-22 17:08:28--  https://uc5e0c03b1fd422e4efdf9b56c46.dl.dropboxusercontent.com/cd/0/inline2/Auv2moV75j6bD1ZVWnQ8TzgrCT8YBavOXNvuI93KzHABa5foU5NHalPz2iQVfVIxJqmlUNryF9fCadSY8v0pFlH56XP593-KMVw74ZrrfGxJx7cylo6nofaqbN6VgrPRTVrHRCmsZN5jvmf_tjdUSF3AbS5eYHVZFyJk7hjbVMW4nOEc2R5qNXUtHfBhgNnPVALcCzygAzHAPG85VZPW-lEbnVPgCybPUM-cNsxBEKOGQJtGIAAelpJyJpppUuIIr3hgXBHKLylw-29RpcKkeADZ3MJHzAIEnXehRGlzhe7aYzWquPr6-Pqi82AWy9sieCEC2m8xAWKZgpWG7Vw5-F1LZpihT2zqET2lfb6JtpP4bg/file
Reusing existing connection to uc5e0c03b1fd422e4efdf9b56c46.dl.dropboxusercontent.com:443.
HTTP request sent, awaiting response... 200 OK
Length: 187049523 (178M) [application/zip]
Saving to: CamVid.zip

CamVid.zip          100%[===================&gt;] 178.38M  25.0MB/s    in 8.3s    

2019-12-22 17:08:37 (21.5 MB/s) - CamVid.zip saved [187049523/187049523]

/notebooks</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch, os</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> glob <span class="im">import</span> glob</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> time <span class="im">import</span> time</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms <span class="im">import</span> ToTensor, ToPILImage</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="dataset-pipeline" class="level2">
<h2 class="anchored" data-anchor-id="dataset-pipeline">Dataset Pipeline</h2>
<section id="camvid-dataset" class="level3">
<h3 class="anchored" data-anchor-id="camvid-dataset">Camvid Dataset</h3>
<p>The <code>CamVidDataset</code> characterizes the key features of the dataset that we want to generate on the fly. inherits from <code>torch.utils.data.Dataset</code> in order to leverage the multiprocessing functionalities of Pytorch.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CamVidDataset(Dataset):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, images, labels, height, width):</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.images <span class="op">=</span> images</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.labels <span class="op">=</span> labels</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.height <span class="op">=</span> height</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.width <span class="op">=</span> width</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.images)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, index):</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>        image_id <span class="op">=</span> <span class="va">self</span>.images[index]</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>        label_id <span class="op">=</span> <span class="va">self</span>.labels[index]</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Read Image</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> Image.<span class="bu">open</span>(image_id)</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> [np.array(x)]</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> np.stack(x, axis<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.tensor(x).transpose(<span class="dv">0</span>, <span class="dv">2</span>).transpose(<span class="dv">1</span>, <span class="dv">3</span>) <span class="co"># Convert to N, C, H, W</span></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Read Mask</span></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> Image.<span class="bu">open</span>(label_id)</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> [np.array(y)]</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> torch.tensor(y)</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x.squeeze(), y.squeeze()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Get the image file lists</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>train_images <span class="op">=</span> <span class="bu">sorted</span>(glob(<span class="st">'./camvid/train/*'</span>))</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>train_labels <span class="op">=</span> <span class="bu">sorted</span>(glob(<span class="st">'./camvid/trainannot/*'</span>))</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>val_images <span class="op">=</span> <span class="bu">sorted</span>(glob(<span class="st">'./camvid/val/*'</span>))</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>val_labels <span class="op">=</span> <span class="bu">sorted</span>(glob(<span class="st">'./camvid/valannot/*'</span>))</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>test_images <span class="op">=</span> <span class="bu">sorted</span>(glob(<span class="st">'./camvid/test/*'</span>))</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>test_labels <span class="op">=</span> <span class="bu">sorted</span>(glob(<span class="st">'./camvid/testannot/*'</span>))</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">10</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Define the <code>CamVidDataset</code> Objects</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> CamVidDataset(train_images, train_labels, <span class="dv">512</span>, <span class="dv">512</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>val_dataset <span class="op">=</span> CamVidDataset(val_images, val_labels, <span class="dv">512</span>, <span class="dv">512</span>)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> CamVidDataset(test_images, test_labels, <span class="dv">512</span>, <span class="dv">512</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we would create the <code>DataLoader</code> objects which would generate data from the dataset objects. The arguments that we would set here are:</p>
<ul>
<li><code>batch_size</code>: this denotes the number of samples contained in each generated batch.</li>
<li><code>shuffle</code>: If set to <code>True</code>, we will get a new order of exploration at each pass (or just keep a linear exploration scheme otherwise). Shuffling the order in which examples are fed to the classifier is helpful so that batches between epochs do not look alike. Doing so will eventually make our model more robust.</li>
<li><code>num_workers</code>: this denotes the number of processes that generate batches in parallel. A high enough number of workers assures that CPU computations are efficiently managed, i.e.&nbsp;that the bottleneck is indeed the neural networks forward and backward operations on the GPU (and not data generation).</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> DataLoader(val_dataset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> DataLoader(test_dataset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code>decode_segmap</code> function accepts an image of shape <code>(H, W)</code> and a color dictionary denoting the <code>BGR</code> color codes to various objects in order for us to visualize the segmentation Masks.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> decode_segmap(image, color_dict):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    label_colours <span class="op">=</span> np.array([</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>        color_dict[<span class="st">'obj0'</span>], color_dict[<span class="st">'obj1'</span>],</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>        color_dict[<span class="st">'obj2'</span>], color_dict[<span class="st">'obj3'</span>],</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>        color_dict[<span class="st">'obj4'</span>], color_dict[<span class="st">'obj5'</span>],</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>        color_dict[<span class="st">'obj6'</span>], color_dict[<span class="st">'obj7'</span>],</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>        color_dict[<span class="st">'obj8'</span>], color_dict[<span class="st">'obj9'</span>],</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>        color_dict[<span class="st">'obj10'</span>], color_dict[<span class="st">'obj11'</span>]</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>    ]).astype(np.uint8)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> np.zeros_like(image).astype(np.uint8)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>    g <span class="op">=</span> np.zeros_like(image).astype(np.uint8)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> np.zeros_like(image).astype(np.uint8)</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">12</span>):</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>        r[image <span class="op">==</span> l] <span class="op">=</span> label_colours[l, <span class="dv">0</span>]</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>        g[image <span class="op">==</span> l] <span class="op">=</span> label_colours[l, <span class="dv">1</span>]</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>        b[image <span class="op">==</span> l] <span class="op">=</span> label_colours[l, <span class="dv">2</span>]</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>    rgb <span class="op">=</span> np.zeros((image.shape[<span class="dv">0</span>], image.shape[<span class="dv">1</span>], <span class="dv">3</span>)).astype(np.uint8)</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>    rgb[:, :, <span class="dv">0</span>] <span class="op">=</span> b</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>    rgb[:, :, <span class="dv">1</span>] <span class="op">=</span> g</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>    rgb[:, :, <span class="dv">2</span>] <span class="op">=</span> r</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> rgb</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code>predict_rgb</code> function takes the model(enet in our case), a tensor denoting a single image in the form <code>(1, C, H, W)</code> and the color_dict and gives us the visualizable prediction</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict_rgb(model, tensor, color_dict):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> model(tensor.<span class="bu">float</span>()).squeeze(<span class="dv">0</span>)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> out.data.<span class="bu">max</span>(<span class="dv">0</span>)[<span class="dv">1</span>].cpu().numpy()</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> decode_segmap(out, color_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code>color_dict</code> is a dictionary where each object is mapped to its respective color code.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>color_dict <span class="op">=</span> {</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'obj0'</span> : [<span class="dv">255</span>, <span class="dv">0</span>, <span class="dv">0</span>], <span class="co"># Sky</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'obj1'</span> : [<span class="dv">0</span>, <span class="dv">51</span>, <span class="dv">204</span>], <span class="co"># Building</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'obj2'</span> : [<span class="dv">0</span>, <span class="dv">255</span>, <span class="dv">255</span>], <span class="co"># Posts</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'obj3'</span> : [<span class="dv">153</span>, <span class="dv">102</span>, <span class="dv">102</span>], <span class="co"># Road</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'obj4'</span> : [<span class="dv">51</span>, <span class="dv">0</span>, <span class="dv">102</span>], <span class="co"># Pavement</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'obj5'</span> : [<span class="dv">0</span>, <span class="dv">255</span>, <span class="dv">0</span>], <span class="co"># Trees</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'obj6'</span> : [<span class="dv">102</span>, <span class="dv">153</span>, <span class="dv">153</span>], <span class="co"># Signs</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'obj7'</span> : [<span class="dv">204</span>, <span class="dv">0</span>, <span class="dv">102</span>], <span class="co"># Fence</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'obj8'</span> : [<span class="dv">102</span>, <span class="dv">0</span>, <span class="dv">0</span>], <span class="co"># Car</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'obj9'</span> : [<span class="dv">0</span>, <span class="dv">153</span>, <span class="dv">102</span>], <span class="co"># Pedestrian</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'obj10'</span> : [<span class="dv">255</span>, <span class="dv">255</span>, <span class="dv">255</span>], <span class="co"># Cyclist</span></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'obj11'</span> : [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>] <span class="co"># bicycles</span></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let us generate a batch from the train dataloader and visualize them along with their prediction using an untrained Enet.</p>
<div class="cell" data-outputid="dfae1928-748c-4326-a724-5a76cab9bfc5">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>x_batch, y_batch <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_loader))</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>x_batch.shape, y_batch.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>(torch.Size([10, 3, 360, 480]), torch.Size([10, 360, 480]))</code></pre>
</div>
</div>
<div class="cell" data-outputid="e4340f9b-2ef3-43f9-c27e-9e32aed0591c">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(nrows <span class="op">=</span> <span class="dv">4</span>, ncols <span class="op">=</span> <span class="dv">3</span>, figsize <span class="op">=</span> (<span class="dv">16</span>, <span class="dv">16</span>))</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>plt.setp(axes.flat, xticks <span class="op">=</span> [], yticks <span class="op">=</span> [])</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, ax <span class="kw">in</span> <span class="bu">enumerate</span>(axes.flat):</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">3</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>        ax.imshow(ToPILImage()(x_batch[c]))</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>        ax.set_xlabel(<span class="st">'Image_'</span> <span class="op">+</span> <span class="bu">str</span>(c))</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> i <span class="op">%</span> <span class="dv">3</span> <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>        ax.imshow(decode_segmap(y_batch[c], color_dict))</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>        ax.set_xlabel(<span class="st">'Ground_Truth_'</span> <span class="op">+</span> <span class="bu">str</span>(c))</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> i <span class="op">%</span> <span class="dv">3</span> <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>        ax.imshow(predict_rgb(enet, x_batch[c].unsqueeze(<span class="dv">0</span>).to(device), color_dict))</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>        ax.set_xlabel(<span class="st">'Predicted_Mask_'</span> <span class="op">+</span> <span class="bu">str</span>(c))</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>        c <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="enet-camvid_files/figure-html/cell-25-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="training" class="level2">
<h2 class="anchored" data-anchor-id="training">Training</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim <span class="im">import</span> Adam</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> CrossEntropyLoss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The authors make use of a custom class weighing scheme defined as <span class="math inline">\(w_{class} = \frac{1}{ln(c + p_{class})}\)</span>, where <code>c</code> is an additional hyper-parameter set to <code>1.02</code>. The advantage of this weighing strategy is that in contrast to the inverse class probability weighing strategy, the weights are bounded as the probability approaches 0.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_class_weights(loader, num_classes, c<span class="op">=</span><span class="fl">1.02</span>):</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    _, y<span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(loader))</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    y_flat <span class="op">=</span> y.flatten()</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    each_class <span class="op">=</span> np.bincount(y_flat, minlength<span class="op">=</span>num_classes)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    p_class <span class="op">=</span> each_class <span class="op">/</span> <span class="bu">len</span>(y_flat)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (np.log(c <span class="op">+</span> p_class))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we will set up the Criterion and Optimizer. The learning rate is set to <code>5e-4</code> with a weight decay of <code>2e-4</code> as mentioned in the paper.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>class_weights <span class="op">=</span> get_class_weights(train_loader, <span class="dv">12</span>)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> CrossEntropyLoss(</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    weight<span class="op">=</span>torch.FloatTensor(class_weights).to(device)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> Adam(</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>    enet.parameters(),</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>    lr<span class="op">=</span><span class="fl">5e-4</span>,</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>    weight_decay<span class="op">=</span><span class="fl">2e-4</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we implement the training procedure:</p>
<ol type="1">
<li>We first loop over the Main Training Loop for a particular number of epochs.</li>
<li>For each epoch, we loop over the dataset for a particular number of steps which is equal to <code>length of dataset // batch_size</code>. This is to ensure that the model gets a chance to look at most of the images in a single epoch.</li>
<li>In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.</li>
<li>Perform Backpropagation.</li>
<li>Store the training loss.</li>
<li>Log the traning results (optional).</li>
<li>Perform Validation using the validation dataloader.</li>
<li>Log the validation results (optional).</li>
<li>Save the model states and results after several epochs (optional).</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    model, train_dataloader, val_dataloader,</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    device, criterion, optimizer, train_step_size, val_step_size,</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    visualize_every, save_every, save_location, save_prefix, epochs):</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Make sure that the checkpoint location exists</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>        os.mkdir(save_location)</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span>:</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>    train_loss_history, val_loss_history <span class="op">=</span> [], []</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Training</span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, epochs <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'Epoch </span><span class="sc">{}</span><span class="ch">\n</span><span class="st">'</span>.<span class="bu">format</span>(epoch))</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Training</span></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>        start <span class="op">=</span> time()</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>        model.train()</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step Loop</span></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> step <span class="kw">in</span> tqdm(<span class="bu">range</span>(train_step_size)):</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>            x_batch, y_batch <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_dataloader))</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>            x_batch <span class="op">=</span> x_batch.squeeze().to(device)</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>            y_batch <span class="op">=</span> y_batch.squeeze().to(device)</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> model(x_batch.<span class="bu">float</span>())</span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(out, y_batch.<span class="bu">long</span>())</span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a>            train_loss <span class="op">+=</span> loss.item()</span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a>        train_loss_history.append(train_loss <span class="op">/</span> train_step_size)</span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">Training Loss: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(train_loss_history[<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'Training Time: </span><span class="sc">{}</span><span class="st"> seconds'</span>.<span class="bu">format</span>(time() <span class="op">-</span> start))</span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Validation</span></span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a>        val_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true" tabindex="-1"></a>        model.<span class="bu">eval</span>()</span>
<span id="cb31-35"><a href="#cb31-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> step <span class="kw">in</span> tqdm(<span class="bu">range</span>(val_step_size)):</span>
<span id="cb31-36"><a href="#cb31-36" aria-hidden="true" tabindex="-1"></a>            x_val, y_val <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(val_dataloader))</span>
<span id="cb31-37"><a href="#cb31-37" aria-hidden="true" tabindex="-1"></a>            x_val <span class="op">=</span> x_val.squeeze().to(device)</span>
<span id="cb31-38"><a href="#cb31-38" aria-hidden="true" tabindex="-1"></a>            y_val <span class="op">=</span> y_val.squeeze().to(device)</span>
<span id="cb31-39"><a href="#cb31-39" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> model(x_val.<span class="bu">float</span>())</span>
<span id="cb31-40"><a href="#cb31-40" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> out.data.<span class="bu">max</span>(<span class="dv">1</span>)[<span class="dv">1</span>]</span>
<span id="cb31-41"><a href="#cb31-41" aria-hidden="true" tabindex="-1"></a>            val_loss <span class="op">+=</span> (y_val.<span class="bu">long</span>() <span class="op">-</span> out.<span class="bu">long</span>()).<span class="bu">float</span>().mean()</span>
<span id="cb31-42"><a href="#cb31-42" aria-hidden="true" tabindex="-1"></a>        val_loss_history.append(val_loss)</span>
<span id="cb31-43"><a href="#cb31-43" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">Validation Loss: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(val_loss))</span>
<span id="cb31-44"><a href="#cb31-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Visualization</span></span>
<span id="cb31-45"><a href="#cb31-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> epoch <span class="op">%</span> visualize_every <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb31-46"><a href="#cb31-46" aria-hidden="true" tabindex="-1"></a>            x_batch, y_batch <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_loader))</span>
<span id="cb31-47"><a href="#cb31-47" aria-hidden="true" tabindex="-1"></a>            fig, axes <span class="op">=</span> plt.subplots(nrows <span class="op">=</span> <span class="dv">4</span>, ncols <span class="op">=</span> <span class="dv">3</span>, figsize <span class="op">=</span> (<span class="dv">16</span>, <span class="dv">16</span>))</span>
<span id="cb31-48"><a href="#cb31-48" aria-hidden="true" tabindex="-1"></a>            plt.setp(axes.flat, xticks <span class="op">=</span> [], yticks <span class="op">=</span> [])</span>
<span id="cb31-49"><a href="#cb31-49" aria-hidden="true" tabindex="-1"></a>            c <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb31-50"><a href="#cb31-50" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i, ax <span class="kw">in</span> <span class="bu">enumerate</span>(axes.flat):</span>
<span id="cb31-51"><a href="#cb31-51" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> i <span class="op">%</span> <span class="dv">3</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb31-52"><a href="#cb31-52" aria-hidden="true" tabindex="-1"></a>                    ax.imshow(ToPILImage()(x_batch[c]))</span>
<span id="cb31-53"><a href="#cb31-53" aria-hidden="true" tabindex="-1"></a>                    ax.set_xlabel(<span class="st">'Image_'</span> <span class="op">+</span> <span class="bu">str</span>(c))</span>
<span id="cb31-54"><a href="#cb31-54" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> i <span class="op">%</span> <span class="dv">3</span> <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb31-55"><a href="#cb31-55" aria-hidden="true" tabindex="-1"></a>                    ax.imshow(decode_segmap(y_batch[c], color_dict))</span>
<span id="cb31-56"><a href="#cb31-56" aria-hidden="true" tabindex="-1"></a>                    ax.set_xlabel(<span class="st">'Ground_Truth_'</span> <span class="op">+</span> <span class="bu">str</span>(c))</span>
<span id="cb31-57"><a href="#cb31-57" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> i <span class="op">%</span> <span class="dv">3</span> <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb31-58"><a href="#cb31-58" aria-hidden="true" tabindex="-1"></a>                    ax.imshow(predict_rgb(enet, x_batch[c].unsqueeze(<span class="dv">0</span>).to(device), color_dict))</span>
<span id="cb31-59"><a href="#cb31-59" aria-hidden="true" tabindex="-1"></a>                    ax.set_xlabel(<span class="st">'Predicted_Mask_'</span> <span class="op">+</span> <span class="bu">str</span>(c))</span>
<span id="cb31-60"><a href="#cb31-60" aria-hidden="true" tabindex="-1"></a>                    c <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb31-61"><a href="#cb31-61" aria-hidden="true" tabindex="-1"></a>            plt.show()</span>
<span id="cb31-62"><a href="#cb31-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Checkpoints</span></span>
<span id="cb31-63"><a href="#cb31-63" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> epoch <span class="op">%</span> save_every <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb31-64"><a href="#cb31-64" aria-hidden="true" tabindex="-1"></a>            checkpoint <span class="op">=</span> {</span>
<span id="cb31-65"><a href="#cb31-65" aria-hidden="true" tabindex="-1"></a>                <span class="st">'epoch'</span> : epoch,</span>
<span id="cb31-66"><a href="#cb31-66" aria-hidden="true" tabindex="-1"></a>                <span class="st">'train_loss'</span> : train_loss,</span>
<span id="cb31-67"><a href="#cb31-67" aria-hidden="true" tabindex="-1"></a>                <span class="st">'val_loss'</span> : val_loss,</span>
<span id="cb31-68"><a href="#cb31-68" aria-hidden="true" tabindex="-1"></a>                <span class="st">'state_dict'</span> : model.state_dict()</span>
<span id="cb31-69"><a href="#cb31-69" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb31-70"><a href="#cb31-70" aria-hidden="true" tabindex="-1"></a>            torch.save(</span>
<span id="cb31-71"><a href="#cb31-71" aria-hidden="true" tabindex="-1"></a>                checkpoint,</span>
<span id="cb31-72"><a href="#cb31-72" aria-hidden="true" tabindex="-1"></a>                <span class="st">'</span><span class="sc">{}</span><span class="st">/</span><span class="sc">{}</span><span class="st">-</span><span class="sc">{}</span><span class="st">-</span><span class="sc">{}</span><span class="st">-</span><span class="sc">{}</span><span class="st">.pth'</span>.<span class="bu">format</span>(</span>
<span id="cb31-73"><a href="#cb31-73" aria-hidden="true" tabindex="-1"></a>                    save_location, save_prefix,</span>
<span id="cb31-74"><a href="#cb31-74" aria-hidden="true" tabindex="-1"></a>                    epoch, train_loss, val_loss</span>
<span id="cb31-75"><a href="#cb31-75" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb31-76"><a href="#cb31-76" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb31-77"><a href="#cb31-77" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">'Checkpoint saved'</span>)</span>
<span id="cb31-78"><a href="#cb31-78" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(</span>
<span id="cb31-79"><a href="#cb31-79" aria-hidden="true" tabindex="-1"></a>        <span class="st">'</span><span class="ch">\n</span><span class="st">Training Done.</span><span class="ch">\n</span><span class="st">Training Mean Loss: </span><span class="sc">{:6f}</span><span class="ch">\n</span><span class="st">Validation Mean Loss: </span><span class="sc">{:6f}</span><span class="st">'</span>.<span class="bu">format</span>(</span>
<span id="cb31-80"><a href="#cb31-80" aria-hidden="true" tabindex="-1"></a>            <span class="bu">sum</span>(train_loss_history) <span class="op">/</span> epochs,</span>
<span id="cb31-81"><a href="#cb31-81" aria-hidden="true" tabindex="-1"></a>            <span class="bu">sum</span>(val_loss_history) <span class="op">/</span> epochs</span>
<span id="cb31-82"><a href="#cb31-82" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb31-83"><a href="#cb31-83" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb31-84"><a href="#cb31-84" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_loss_history, val_loss_history</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="cd86c412-00e9-4942-8bec-2a9fd2386c26">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>train_loss_history, val_loss_history <span class="op">=</span> train(</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    enet, train_loader, val_loader,</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>    device, criterion, optimizer,</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">len</span>(train_images) <span class="op">//</span> batch_size,</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">len</span>(val_images) <span class="op">//</span> batch_size, <span class="dv">5</span>,</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    <span class="dv">5</span>, <span class="st">'./checkpoints'</span>, <span class="st">'enet-model'</span>, <span class="dv">100</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1


Training Loss: 2.3736738430129156
Training Time: 36.622546672821045 seconds

Validation Loss: 2.495696544647217
Epoch 2


Training Loss: 2.066100193394555
Training Time: 37.32582426071167 seconds

Validation Loss: 1.0354421138763428
Epoch 3


Training Loss: 1.819241699245241
Training Time: 37.225857734680176 seconds

Validation Loss: -5.462801933288574
Epoch 4


Training Loss: 1.6384391950236425
Training Time: 37.67400813102722 seconds

Validation Loss: -6.796051502227783
Epoch 5


Training Loss: 1.5421283278200362
Training Time: 37.320696115493774 seconds

Validation Loss: -4.719521522521973
Checkpoint saved
Epoch 6


Training Loss: 1.4267812801731958
Training Time: 36.2429084777832 seconds

Validation Loss: -4.382036209106445
Epoch 7


Training Loss: 1.3556759390566084
Training Time: 37.677775144577026 seconds

Validation Loss: -7.395351886749268
Epoch 8


Training Loss: 1.3103740215301514
Training Time: 37.39039158821106 seconds

Validation Loss: -6.370512008666992
Epoch 9


Training Loss: 1.2682642042636871
Training Time: 37.26855731010437 seconds

Validation Loss: -8.429301261901855
Epoch 10


Training Loss: 1.2029826508627997
Training Time: 37.28497934341431 seconds

Validation Loss: -6.792354106903076
Checkpoint saved
Epoch 11


Training Loss: 1.1570831570360396
Training Time: 37.477909326553345 seconds

Validation Loss: -6.195928573608398
Epoch 12


Training Loss: 1.1395491758982341
Training Time: 37.90213441848755 seconds

Validation Loss: -5.173197269439697
Epoch 13


Training Loss: 1.1266514195336237
Training Time: 37.404568910598755 seconds

Validation Loss: -5.771578311920166
Epoch 14


Training Loss: 1.0894614275958803
Training Time: 37.46945667266846 seconds

Validation Loss: -5.576180934906006
Epoch 15


Training Loss: 1.0793888171513875
Training Time: 36.50749087333679 seconds

Validation Loss: -6.073637962341309
Checkpoint saved
Epoch 16


Training Loss: 1.0256218943330977
Training Time: 37.52659511566162 seconds

Validation Loss: -8.707944869995117
Epoch 17


Training Loss: 1.0208994067377515
Training Time: 37.60098838806152 seconds

Validation Loss: -6.784101963043213
Epoch 18


Training Loss: 1.00802077849706
Training Time: 37.67285490036011 seconds

Validation Loss: -8.1976957321167
Epoch 19


Training Loss: 0.9760349442561468
Training Time: 37.02203035354614 seconds

Validation Loss: -6.932909965515137
Epoch 20


Training Loss: 0.9686084638039271
Training Time: 36.49009346961975 seconds

Validation Loss: -10.035560607910156
Checkpoint saved
Epoch 21


Training Loss: 0.9232347259918848
Training Time: 36.40699481964111 seconds

Validation Loss: -6.618765830993652
Epoch 22


Training Loss: 0.9214804338084327
Training Time: 37.64718818664551 seconds

Validation Loss: -6.437498569488525
Epoch 23


Training Loss: 0.8775700446632173
Training Time: 36.491459608078 seconds

Validation Loss: -7.585930824279785
Epoch 24


Training Loss: 0.8676599446270201
Training Time: 37.75159788131714 seconds

Validation Loss: -5.8291826248168945
Epoch 25


Training Loss: 0.8542953348822064
Training Time: 38.35563540458679 seconds

Validation Loss: -10.226922988891602
Checkpoint saved
Epoch 26


Training Loss: 0.8472818914386961
Training Time: 36.00928783416748 seconds

Validation Loss: -8.36304759979248
Epoch 27


Training Loss: 0.821869295504358
Training Time: 36.77352595329285 seconds

Validation Loss: -9.58829402923584
Epoch 28


Training Loss: 0.7889702585008409
Training Time: 38.61393928527832 seconds

Validation Loss: -5.412705421447754
Epoch 29


Training Loss: 0.7545491208632787
Training Time: 38.38607335090637 seconds

Validation Loss: -8.005655288696289
Epoch 30


Training Loss: 0.7702976332770454
Training Time: 36.71100616455078 seconds

Validation Loss: -4.231881141662598
Checkpoint saved
Epoch 31


Training Loss: 0.7379177262385687
Training Time: 35.90049076080322 seconds

Validation Loss: -7.388115406036377
Epoch 32


Training Loss: 0.7093521191014184
Training Time: 37.23017859458923 seconds

Validation Loss: -6.115187168121338
Epoch 33


Training Loss: 0.6929942336347368
Training Time: 38.1418399810791 seconds

Validation Loss: -5.854228496551514
Epoch 34


Training Loss: 0.6660703635878034
Training Time: 38.218461751937866 seconds

Validation Loss: -5.8958330154418945
Epoch 35


Training Loss: 0.6632857289579179
Training Time: 37.43135929107666 seconds

Validation Loss: -5.267255783081055
Checkpoint saved
Epoch 36


Training Loss: 0.6700823141468896
Training Time: 39.75525879859924 seconds

Validation Loss: -4.932151794433594
Epoch 37


Training Loss: 0.6306778772009743
Training Time: 37.12487030029297 seconds

Validation Loss: -6.434138774871826
Epoch 38


Training Loss: 0.6451671355300479
Training Time: 36.77497148513794 seconds

Validation Loss: -7.195463180541992
Epoch 39


Training Loss: 0.6210144890679253
Training Time: 37.950018882751465 seconds

Validation Loss: -7.6000566482543945
Epoch 40


Training Loss: 0.6190666158994039
Training Time: 38.053221702575684 seconds

Validation Loss: -5.544920444488525
Checkpoint saved
Epoch 41


Training Loss: 0.5928388577368524
Training Time: 37.63972759246826 seconds

Validation Loss: -6.879909992218018
Epoch 42


Training Loss: 0.5870317411091592
Training Time: 37.024226903915405 seconds

Validation Loss: -6.742264270782471
Epoch 43


Training Loss: 0.5675398872958289
Training Time: 37.81453609466553 seconds

Validation Loss: -5.535061359405518
Epoch 44


Training Loss: 0.5501133882337146
Training Time: 36.879273414611816 seconds

Validation Loss: -4.253548622131348
Epoch 45


Training Loss: 0.5503247496154573
Training Time: 37.58225345611572 seconds

Validation Loss: -5.902456283569336
Checkpoint saved
Epoch 46


Training Loss: 0.5512463673949242
Training Time: 36.606677532196045 seconds

Validation Loss: -5.735024452209473
Epoch 47


Training Loss: 0.544807703130775
Training Time: 38.517911434173584 seconds

Validation Loss: -5.34906005859375
Epoch 48


Training Loss: 0.5279948107070394
Training Time: 36.94304895401001 seconds

Validation Loss: -5.252256393432617
Epoch 49


Training Loss: 0.5191673007276323
Training Time: 37.99461269378662 seconds

Validation Loss: -7.820833206176758
Epoch 50


Training Loss: 0.49875519176324207
Training Time: 39.174588203430176 seconds

Validation Loss: -4.738555431365967
Checkpoint saved
Epoch 51


Training Loss: 0.5172228713830312
Training Time: 37.38908004760742 seconds

Validation Loss: -4.111539840698242
Epoch 52


Training Loss: 0.5180092445678182
Training Time: 37.96890068054199 seconds

Validation Loss: -7.410361289978027
Epoch 53


Training Loss: 0.5053947452041838
Training Time: 38.50911474227905 seconds

Validation Loss: -5.627304553985596
Epoch 54


Training Loss: 0.4888179575403531
Training Time: 38.64139103889465 seconds

Validation Loss: -5.422050952911377
Epoch 55


Training Loss: 0.4791935044858191
Training Time: 37.589763879776 seconds

Validation Loss: -5.576418876647949
Checkpoint saved
Epoch 56


Training Loss: 0.45455337150229347
Training Time: 40.3157172203064 seconds

Validation Loss: -5.987201690673828
Epoch 57


Training Loss: 0.4661391567852762
Training Time: 36.715943336486816 seconds

Validation Loss: -4.632068157196045
Epoch 58


Training Loss: 0.4665369697742992
Training Time: 36.19537806510925 seconds

Validation Loss: -5.786327838897705
Epoch 59


Training Loss: 0.45907818608813816
Training Time: 38.01468300819397 seconds

Validation Loss: -6.478023529052734
Epoch 60


Training Loss: 0.4500412220756213
Training Time: 37.21549987792969 seconds

Validation Loss: -5.177182674407959
Checkpoint saved
Epoch 61


Training Loss: 0.45753948307699627
Training Time: 38.74998998641968 seconds

Validation Loss: -5.799277305603027
Epoch 62


Training Loss: 0.42691650407181847
Training Time: 37.344823598861694 seconds

Validation Loss: -4.130724906921387
Epoch 63


Training Loss: 0.4300544717245632
Training Time: 37.420971632003784 seconds

Validation Loss: -4.042947769165039
Epoch 64


Training Loss: 0.42879077792167664
Training Time: 37.66694641113281 seconds

Validation Loss: -4.180288314819336
Epoch 65


Training Loss: 0.4272735383775499
Training Time: 37.58568787574768 seconds

Validation Loss: -4.429321765899658
Checkpoint saved
Epoch 66


Training Loss: 0.42417974604500663
Training Time: 37.05702567100525 seconds

Validation Loss: -4.533682346343994
Epoch 67


Training Loss: 0.4159412756562233
Training Time: 37.56980299949646 seconds

Validation Loss: -5.018970012664795
Epoch 68


Training Loss: 0.4148317939705319
Training Time: 37.138564348220825 seconds

Validation Loss: -3.9047155380249023
Epoch 69


Training Loss: 0.3963303491473198
Training Time: 37.7933931350708 seconds

Validation Loss: -5.635763168334961
Epoch 70


Training Loss: 0.40438303930891883
Training Time: 37.2310631275177 seconds

Validation Loss: -4.414734363555908
Checkpoint saved
Epoch 71


Training Loss: 0.39953534967369503
Training Time: 36.96225690841675 seconds

Validation Loss: -3.7862653732299805
Epoch 72


Training Loss: 0.39342257877190906
Training Time: 38.77004361152649 seconds

Validation Loss: -4.276852130889893
Epoch 73


Training Loss: 0.3897610389524036
Training Time: 38.38589859008789 seconds

Validation Loss: -4.030964374542236
Epoch 74


Training Loss: 0.405379969212744
Training Time: 36.81379222869873 seconds

Validation Loss: -5.7940850257873535
Epoch 75


Training Loss: 0.377888491584195
Training Time: 37.3532612323761 seconds

Validation Loss: -3.9622135162353516
Checkpoint saved
Epoch 76


Training Loss: 0.37682998263173634
Training Time: 36.6818642616272 seconds

Validation Loss: -5.148308753967285
Epoch 77


Training Loss: 0.371006368762917
Training Time: 38.503026723861694 seconds

Validation Loss: -3.861609935760498
Epoch 78


Training Loss: 0.3602796254886521
Training Time: 38.01511263847351 seconds

Validation Loss: -3.9753198623657227
Epoch 79


Training Loss: 0.3653753134939406
Training Time: 37.917211055755615 seconds

Validation Loss: -2.8377439975738525
Epoch 80


Training Loss: 0.37505776186784107
Training Time: 38.310165882110596 seconds

Validation Loss: -5.334249973297119
Checkpoint saved
Epoch 81


Training Loss: 0.36951222353511387
Training Time: 38.20164632797241 seconds

Validation Loss: -4.66939640045166
Epoch 82


Training Loss: 0.35576848520172966
Training Time: 37.462151527404785 seconds

Validation Loss: -4.722535610198975
Epoch 83


Training Loss: 0.36002279900842243
Training Time: 37.20385551452637 seconds

Validation Loss: -4.74808406829834
Epoch 84


Training Loss: 0.3489703999625312
Training Time: 38.138071060180664 seconds

Validation Loss: -4.982658386230469
Epoch 85


Training Loss: 0.34990982214609784
Training Time: 37.759665727615356 seconds

Validation Loss: -4.157069683074951
Checkpoint saved
Epoch 86


Training Loss: 0.35239921179082656
Training Time: 37.88401412963867 seconds

Validation Loss: -4.5902533531188965
Epoch 87


Training Loss: 0.34805963353978264
Training Time: 37.58679533004761 seconds

Validation Loss: -4.319746971130371
Epoch 88


Training Loss: 0.3448881424135632
Training Time: 37.084150552749634 seconds

Validation Loss: -3.250303268432617
Epoch 89


Training Loss: 0.3393386834197574
Training Time: 38.44252109527588 seconds

Validation Loss: -3.880829334259033
Epoch 90


Training Loss: 0.3295273143384192
Training Time: 37.59496545791626 seconds

Validation Loss: -4.594930648803711
Checkpoint saved
Epoch 91


Training Loss: 0.3464961921175321
Training Time: 36.96203589439392 seconds

Validation Loss: -3.407609224319458
Epoch 92


Training Loss: 0.34060925907558864
Training Time: 38.37405014038086 seconds

Validation Loss: -4.496046543121338
Epoch 93


Training Loss: 0.3336273547675874
Training Time: 36.53633213043213 seconds

Validation Loss: -2.9917984008789062
Epoch 94


Training Loss: 0.3360065817832947
Training Time: 37.400681495666504 seconds

Validation Loss: -4.259033679962158
Epoch 95


Training Loss: 0.33179697228802574
Training Time: 37.88106441497803 seconds

Validation Loss: -6.045409202575684
Checkpoint saved
Epoch 96


Training Loss: 0.329134587612417
Training Time: 38.55759382247925 seconds

Validation Loss: -4.940917015075684
Epoch 97


Training Loss: 0.3220530201991399
Training Time: 37.08751201629639 seconds

Validation Loss: -4.636651992797852
Epoch 98


Training Loss: 0.31848980320824516
Training Time: 37.68484568595886 seconds

Validation Loss: -2.7739005088806152
Epoch 99


Training Loss: 0.32071789933575523
Training Time: 37.38284635543823 seconds

Validation Loss: -3.3696560859680176
Epoch 100


Training Loss: 0.3072536442842748
Training Time: 36.655545473098755 seconds

Validation Loss: -3.19159197807312
Checkpoint saved

Training Done.
Training Mean Loss: 0.667980
Validation Mean Loss: -5.383368</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="enet-camvid_files/figure-html/cell-30-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="enet-camvid_files/figure-html/cell-30-output-3.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="enet-camvid_files/figure-html/cell-30-output-4.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="enet-camvid_files/figure-html/cell-30-output-5.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="enet-camvid_files/figure-html/cell-30-output-6.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="enet-camvid_files/figure-html/cell-30-output-7.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="enet-camvid_files/figure-html/cell-30-output-8.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="enet-camvid_files/figure-html/cell-30-output-9.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="enet-camvid_files/figure-html/cell-30-output-10.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="enet-camvid_files/figure-html/cell-30-output-11.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="enet-camvid_files/figure-html/cell-30-output-12.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="enet-camvid_files/figure-html/cell-30-output-13.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="enet-camvid_files/figure-html/cell-30-output-14.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="enet-camvid_files/figure-html/cell-30-output-15.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="enet-camvid_files/figure-html/cell-30-output-16.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="enet-camvid_files/figure-html/cell-30-output-17.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="enet-camvid_files/figure-html/cell-30-output-18.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="enet-camvid_files/figure-html/cell-30-output-19.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="enet-camvid_files/figure-html/cell-30-output-20.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="enet-camvid_files/figure-html/cell-30-output-21.png" class="img-fluid"></p>
</div>
</div>
<p>Now, let us visualize the results</p>
<div class="cell" data-outputid="c1c14bdb-c795-4eaa-e6c7-723611d1aa07">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>plt.plot(train_loss_history, color <span class="op">=</span> <span class="st">'b'</span>, label <span class="op">=</span> <span class="st">'Training Loss'</span>)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>plt.plot(val_loss_history, color <span class="op">=</span> <span class="st">'r'</span>, label <span class="op">=</span> <span class="st">'Validation Loss'</span>)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="enet-camvid_files/figure-html/cell-31-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-outputid="21f1c1bb-891c-44d3-f352-5e2f1bc058db">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>plt.plot(train_loss_history, color <span class="op">=</span> <span class="st">'b'</span>, label <span class="op">=</span> <span class="st">'Training Loss'</span>)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="enet-camvid_files/figure-html/cell-32-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-outputid="b4fb7ae1-dffd-462e-f77c-9a3ab032e85e">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>plt.plot(val_loss_history, color <span class="op">=</span> <span class="st">'r'</span>, label <span class="op">=</span> <span class="st">'Validation Loss'</span>)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="enet-camvid_files/figure-html/cell-33-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="prediction" class="level2">
<h2 class="anchored" data-anchor-id="prediction">Prediction</h2>
<p>We will be predicting with the weights at epoch 65 where both training and validation loss seems to be stable. This is done in order to avoid overfitting.</p>
<div class="cell" data-outputid="a357e1b7-7d55-41fc-86bc-e8a66dc123bf">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>state_dict <span class="op">=</span> torch.load(<span class="st">'./checkpoints/enet-model-65-14.726004391908646--3.9436190128326416.pth'</span>)[<span class="st">'state_dict'</span>]</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>enet.load_state_dict(state_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>&lt;All keys matched successfully&gt;</code></pre>
</div>
</div>
<p>Prediction on Training Data</p>
<div class="cell" data-outputid="4923d086-7730-4e46-869a-76fb871c669f">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>x_batch, y_batch <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_loader))</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(nrows <span class="op">=</span> <span class="dv">4</span>, ncols <span class="op">=</span> <span class="dv">3</span>, figsize <span class="op">=</span> (<span class="dv">16</span>, <span class="dv">16</span>))</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>plt.setp(axes.flat, xticks <span class="op">=</span> [], yticks <span class="op">=</span> [])</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, ax <span class="kw">in</span> <span class="bu">enumerate</span>(axes.flat):</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">3</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>        ax.imshow(ToPILImage()(x_batch[c]))</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>        ax.set_xlabel(<span class="st">'Image_'</span> <span class="op">+</span> <span class="bu">str</span>(c))</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> i <span class="op">%</span> <span class="dv">3</span> <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>        ax.imshow(decode_segmap(y_batch[c], color_dict))</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>        ax.set_xlabel(<span class="st">'Ground_Truth_'</span> <span class="op">+</span> <span class="bu">str</span>(c))</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> i <span class="op">%</span> <span class="dv">3</span> <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>        ax.imshow(predict_rgb(enet, x_batch[c].unsqueeze(<span class="dv">0</span>).to(device), color_dict))</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>        ax.set_xlabel(<span class="st">'Predicted_Mask_'</span> <span class="op">+</span> <span class="bu">str</span>(c))</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>        c <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="enet-camvid_files/figure-html/cell-35-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Prediction on Validation Data</p>
<div class="cell" data-outputid="f2092bc8-df43-4c1e-83df-4150ce1b4ac4">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>x_batch, y_batch <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(val_loader))</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(nrows <span class="op">=</span> <span class="dv">4</span>, ncols <span class="op">=</span> <span class="dv">3</span>, figsize <span class="op">=</span> (<span class="dv">16</span>, <span class="dv">16</span>))</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>plt.setp(axes.flat, xticks <span class="op">=</span> [], yticks <span class="op">=</span> [])</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, ax <span class="kw">in</span> <span class="bu">enumerate</span>(axes.flat):</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">3</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>        ax.imshow(ToPILImage()(x_batch[c]))</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>        ax.set_xlabel(<span class="st">'Image_'</span> <span class="op">+</span> <span class="bu">str</span>(c))</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> i <span class="op">%</span> <span class="dv">3</span> <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>        ax.imshow(decode_segmap(y_batch[c], color_dict))</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>        ax.set_xlabel(<span class="st">'Ground_Truth_'</span> <span class="op">+</span> <span class="bu">str</span>(c))</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> i <span class="op">%</span> <span class="dv">3</span> <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>        ax.imshow(predict_rgb(enet, x_batch[c].unsqueeze(<span class="dv">0</span>).to(device), color_dict))</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>        ax.set_xlabel(<span class="st">'Predicted_Mask_'</span> <span class="op">+</span> <span class="bu">str</span>(c))</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>        c <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="enet-camvid_files/figure-html/cell-36-output-1.png" class="img-fluid"></p>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>